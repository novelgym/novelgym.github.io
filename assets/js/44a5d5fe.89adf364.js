"use strict";(self.webpackChunkng_website=self.webpackChunkng_website||[]).push([[176],{3108:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>l});var t=a(5893),r=a(1151);const i={sidebar_position:2},s="Combining Planning & RL Agents",o={id:"agent/combining",title:"Combining Planning & RL Agents",description:"The agent training architecture is made up of the single agent wrapper, the neurosymbolic wrapper, and the reward-shaping wrapper, all of which can be found in the envs folder. The wrappers responsible for combining the symbolic planning and reinforcement learning agents are the neurosymbolic wrapper and the reward-shaping wrapper.",source:"@site/docs/agent/combining.md",sourceDirName:"agent",slug:"/agent/combining",permalink:"/docs/agent/combining",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Defining Spaces",permalink:"/docs/agent/spaces"},next:{title:"Citation",permalink:"/docs/citation"}},c={},l=[{value:"Changing Reward Based on Planner",id:"changing-reward-based-on-planner",level:2},{value:"Switching Between Planner &amp; External Agent",id:"switching-between-planner--external-agent",level:2},{value:"Benchmark Results",id:"benchmark-results",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",img:"img",p:"p",...(0,r.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"combining-planning--rl-agents",children:"Combining Planning & RL Agents"}),"\n",(0,t.jsxs)(n.p,{children:["The agent training architecture is made up of the single agent wrapper, the neurosymbolic wrapper, and the reward-shaping wrapper, all of which can be found in the ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/tree/main/envs",children:"envs"})," folder. The wrappers responsible for combining the symbolic planning and reinforcement learning agents are the neurosymbolic wrapper and the reward-shaping wrapper."]}),"\n",(0,t.jsxs)(n.p,{children:["In this part of the tutorial, we discuss the two existing case studies, ",(0,t.jsx)(n.a,{href:"#changing-reward-based-on-planner",children:"Changing Reward Based on Planner"})," and ",(0,t.jsx)(n.a,{href:"#switching-between-planner--external-agent",children:"Switching Between Planner & External Agent"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"changing-reward-based-on-planner",children:"Changing Reward Based on Planner"}),"\n",(0,t.jsx)(n.p,{children:"For reward shaping, the wrapper modifies the reward function based on the domain knowledge in PDDL and the plan given by the planner."}),"\n",(0,t.jsx)(n.h2,{id:"switching-between-planner--external-agent",children:"Switching Between Planner & External Agent"}),"\n",(0,t.jsx)(n.p,{children:"For RapidLearn, the fast_forward function will automatically execute the time steps for RL until no plan could be found or an action failed."}),"\n",(0,t.jsx)(n.h2,{id:"benchmark-results",children:"Benchmark Results"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Benchmark",src:a(6111).Z+"",width:"1653",height:"445"})})]})}function h(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},6111:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/benchmark-7a34472d4816cf28c7851bd68a5cd0cd.png"},1151:(e,n,a)=>{a.d(n,{Z:()=>o,a:()=>s});var t=a(7294);const r={},i=t.createContext(r);function s(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);
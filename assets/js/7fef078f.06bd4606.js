"use strict";(self.webpackChunkng_website=self.webpackChunkng_website||[]).push([[316],{1016:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>l});var o=t(5893),i=t(1151);const r={sidebar_position:1},s="Keyboard Demo",a={id:"environment/demo",title:"Keyboard Demo",description:"To get a feel for the game the agent will learn, run the following command from the root of the NovelGym repository.",source:"@site/docs/environment/demo.md",sourceDirName:"environment",slug:"/environment/demo",permalink:"/docs/environment/demo",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Customizing Environments",permalink:"/docs/category/customizing-environments"},next:{title:"Creating Your Environment",permalink:"/docs/environment/custom"}},c={},l=[];function d(e){const n={a:"a",code:"code",h1:"h1",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"keyboard-demo",children:"Keyboard Demo"}),"\n",(0,o.jsxs)(n.p,{children:["To get a feel for the game the agent will learn, run the following command from the root of the ",(0,o.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym",children:"NovelGym"})," repository."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"python manual_novelty_test1.py --novelty none --env sa\n"})}),"\n",(0,o.jsx)(n.p,{children:"A window like the one below will pop up, rendering the environment and the agent navigating it."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Environment",src:t(5146).Z+"",width:"556",height:"378"})}),"\n",(0,o.jsxs)(n.p,{children:["The red arrow is the agent. You can control it by typing the integer corresponding to the desired action next to the prompt ",(0,o.jsx)(n.code,{children:"action: "})," on the command line. For instance, ",(0,o.jsx)(n.code,{children:"action: 2"}),", which corresponds to ",(0,o.jsx)(n.code,{children:"approach_oak_log"}),", will make the agent move towards the closest oak log."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Tips"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"--novelty none"})," flag means that we are testing the environment without any novelties. Try changing this flag to any of the options listed when running the script with ",(0,o.jsx)(n.code,{children:"--help"}),"."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"--env sa"})," chooses the single agent wrapper. This will be the default environment for the ",(0,o.jsx)(n.a,{href:"../category/customizing-environments",children:"Customizing Environments"})," part of the tutorial. For more on agent wrappers, see the ",(0,o.jsx)(n.a,{href:"../agent/combining",children:"Combining Planning & RL Agents"})," part."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},5146:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/environment-24f5c6d5cd3cd3a6b2d767d7f525a92c.gif"},1151:(e,n,t)=>{t.d(n,{Z:()=>a,a:()=>s});var o=t(7294);const i={},r=o.createContext(i);function s(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);
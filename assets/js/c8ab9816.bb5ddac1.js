"use strict";(self.webpackChunkng_website=self.webpackChunkng_website||[]).push([[77],{7632:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>l});var i=n(5893),o=n(1151);const a={sidebar_position:1},s="Defining Spaces",r={id:"agent/spaces",title:"Defining Spaces",description:"Observation Space",source:"@site/docs/agent/spaces.md",sourceDirName:"agent",slug:"/agent/spaces",permalink:"/docs/agent/spaces",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Training Agents",permalink:"/docs/category/training-agents"},next:{title:"Combining Planning & RL Agents",permalink:"/docs/agent/combining"}},c={},l=[{value:"Observation Space",id:"observation-space",level:2},{value:"Currently Implemented",id:"currently-implemented",level:3},{value:"Adding New",id:"adding-new",level:3},{value:"Action Space",id:"action-space",level:2},{value:"Reward Function",id:"reward-function",level:2}];function d(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",p:"p",...(0,o.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h1,{id:"defining-spaces",children:"Defining Spaces"}),"\n",(0,i.jsx)(t.h2,{id:"observation-space",children:"Observation Space"}),"\n",(0,i.jsxs)(t.p,{children:["The implementations of the observation spaces are in the ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/tree/main/obs_convertion",children:"obs_convertion"})," folder. The base class is ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/base.py",children:"ObservationGenerator"}),", which outlines methods to be implemented in an observation space. See the diagram below for the full class interdependence."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Observations",src:n(1837).Z+"",width:"1703",height:"409"})}),"\n",(0,i.jsx)(t.h3,{id:"currently-implemented",children:"Currently Implemented"}),"\n",(0,i.jsxs)(t.p,{children:["All of the currently implemented observation spaces are gymnasium Box spaces. The ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/lidar_all.py",children:"LidarAll"})," class has a single Box space made from a one-dimensional vector with the item selected by the agent, the agent's inventory, and the euclidean distances to the objects that strike the LiDAR beam sent by the agent in 45 degree increments. The children ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/only_facing.py",children:"OnlyFacingObs"})," and ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/only_hinted.py",children:"NovelOnlyObs"})," limit the number of LiDAR beams sent and the types of objects detected respectively. The child ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/matrix.py",children:"Matrix"})," holds a dictionary with three Box spaces, one for the agent's selected item, one for the agent's inventory, and one for the agent's local view, stored as a two-dimensional square image."]}),"\n",(0,i.jsx)(t.h3,{id:"adding-new",children:"Adding New"}),"\n",(0,i.jsxs)(t.p,{children:["The easiest way of adding a new observation space is by declaring a child of ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/lidar_all.py",children:"LidarAll"})," and overriding those parts of the parent class that differ in the theoretical new observation space. The more complex part is integrating the new space in training. If the structure of the new space differs from that under ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/lidar_all.py",children:"LidarAll"}),", a compatible net must be added to the ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/tree/main/net",children:"net"})," folder, a new policy implemented in the ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/policy_utils.py",children:"policy_utils.py"})," file, and a case added to the ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/train.py",children:"train.py"})," so that the right policy-maker is called instead of ",(0,i.jsx)(t.code,{children:"create_policy"})," when the new observation space is being used."]}),"\n",(0,i.jsxs)(t.p,{children:["Any newly implemented observation space should be placed in the ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/tree/main/obs_convertion",children:"obs_convertion"})," folder, listed in the ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/__init__.py",children:"obs_convertion/__init__.py"})," file consistent with the spaces already there, and included under ",(0,i.jsx)(t.code,{children:"OBS_TYPES"})," in ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/36f78f5e25475a43a8a83627939a5744d0a42c0c/config.py",children:"config.py"})," so that the space can be chosen when training is being run from the command line."]}),"\n",(0,i.jsx)(t.h2,{id:"action-space",children:"Action Space"}),"\n",(0,i.jsxs)(t.p,{children:["The agent's action space is automatically generated from the config file and no advanced modifications are required when integrating new actions. However, one may wish to modify or override the ",(0,i.jsx)(t.code,{children:"action_space"})," method of the ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/envs/single_agent_standard.py",children:"SingleAgentWrapper"})," class, explored in the ",(0,i.jsx)(t.a,{href:"combining",children:"Combining Planning & RL Agents"})," part of the tutorial."]}),"\n",(0,i.jsx)(t.h2,{id:"reward-function",children:"Reward Function"}),"\n",(0,i.jsxs)(t.p,{children:["The reward is generated from the above-described observation module by the ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/36f78f5e25475a43a8a83627939a5744d0a42c0c/envs/single_agent_standard.py",children:"SingleAgentWrapper"})," and passed to the integrated external RL agent. Additional reward generation occurs in the reward-shaping and neurosymbolics wrappers, found in the ",(0,i.jsx)(t.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/tree/main/envs",children:"envs"})," folder and described in detail in the ",(0,i.jsx)(t.a,{href:"combining",children:"Combining Planning & RL Agents"})," part of the tutorial."]})]})}function h(e={}){const{wrapper:t}={...(0,o.a)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},1837:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/Observations.drawio-c38e08243c7ec515815cbea3184b5854.png"},1151:(e,t,n)=>{n.d(t,{Z:()=>r,a:()=>s});var i=n(7294);const o={},a=i.createContext(o);function s(e){const t=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(a.Provider,{value:t},e.children)}}}]);
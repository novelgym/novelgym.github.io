"use strict";(self.webpackChunkng_website=self.webpackChunkng_website||[]).push([[77],{7632:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>a,toc:()=>l});var t=i(5893),s=i(1151);const o={sidebar_position:1},r="Defining Spaces",a={id:"agent/spaces",title:"Defining Spaces",description:"Observation Space",source:"@site/docs/agent/spaces.md",sourceDirName:"agent",slug:"/agent/spaces",permalink:"/docs/agent/spaces",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Training Agents",permalink:"/docs/category/training-agents"},next:{title:"Combining Planning & RL Agents",permalink:"/docs/agent/combining"}},c={},l=[{value:"Observation Space",id:"observation-space",level:2},{value:"Currently Implemented Observation Spaces",id:"currently-implemented-observation-spaces",level:3},{value:"Adding New Observation Spaces",id:"adding-new-observation-spaces",level:3},{value:"Action Space",id:"action-space",level:2},{value:"Reward Function",id:"reward-function",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"defining-spaces",children:"Defining Spaces"}),"\n",(0,t.jsx)(n.h2,{id:"observation-space",children:"Observation Space"}),"\n",(0,t.jsxs)(n.p,{children:["The implementations of the observation spaces are in the ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/tree/main/obs_convertion",children:"obs_convertion"})," folder. The base class is ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/base.py",children:"ObservationGenerator"}),", which outlines methods to be implemented in an observation space. See the diagram below for the full class interdependence."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note"}),": ",(0,t.jsx)(n.code,{children:"ABC"})," refers to Python's ",(0,t.jsx)(n.a,{href:"https://docs.python.org/3.8/library/abc.html",children:"Abstract Base Classes"}),"."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Observations",src:i(1837).Z+"",width:"1703",height:"409"})}),"\n",(0,t.jsx)(n.h3,{id:"currently-implemented-observation-spaces",children:"Currently Implemented Observation Spaces"}),"\n",(0,t.jsxs)(n.p,{children:["All of the currently implemented observation spaces are ",(0,t.jsx)(n.a,{href:"https://gymnasium.farama.org/api/spaces/",children:"gymnasium Box spaces"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/lidar_all.py",children:"LidarAll"})," has a single Box space made from a one-dimensional vector with the item selected by the agent, the agent's inventory, and the euclidean distances to the objects that strike the LiDAR beam sent by the agent in 45 degree increments,"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/only_facing.py",children:"OnlyFacingObs"})," (child of ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/lidar_all.py",children:"LidarAll"}),") limits the number of LiDAR beams sent,"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/only_hinted.py",children:"NovelOnlyObs"})," (child of ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/lidar_all.py",children:"LidarAll"}),") limits the types of objects detected,"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/matrix.py",children:"Matrix"})," (child of ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/lidar_all.py",children:"LidarAll"}),") holds a dictionary with three Box spaces \u2013 one for the agent's selected item, one for the agent's inventory, and one for the agent's local view stored as a two-dimensional square image."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"adding-new-observation-spaces",children:"Adding New Observation Spaces"}),"\n",(0,t.jsx)(n.p,{children:"To integrate a new observation space:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["declare a child of ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/lidar_all.py",children:"LidarAll"})," in the ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/tree/main/obs_convertion",children:"obs_convertion"})," folder and override any parts of the parent class,"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["if the structure of the new space differs from that under ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/lidar_all.py",children:"LidarAll"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["add a compatible net to the ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/tree/main/net",children:"net"})," folder,"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["implement a new policy in the ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/policy_utils.py",children:"policy_utils.py"})," file,"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["add a case to the ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/train.py",children:"train.py"})," file so that the right policy-maker is called instead of ",(0,t.jsx)(n.code,{children:"create_policy"})," when the new observation space is being used,"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["list the new space in the ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/obs_convertion/__init__.py",children:"obs_convertion/__init__.py"})," file consistent with the spaces already there,"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["include the new space under ",(0,t.jsx)(n.code,{children:"OBS_TYPES"})," in ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/36f78f5e25475a43a8a83627939a5744d0a42c0c/config.py",children:"config.py"})," so that the space can be chosen when training is being run from the command line."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"action-space",children:"Action Space"}),"\n",(0,t.jsx)(n.p,{children:"The agent's action space is automatically generated from the config file. Hence, all you need to do to integrate a new action is write the module representing it and reference this module in the config."}),"\n",(0,t.jsxs)(n.p,{children:["Of course, you can also modify or override the ",(0,t.jsx)(n.code,{children:"action_space"})," method of the ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/main/envs/single_agent_standard.py",children:"SingleAgentWrapper"})," class. This class is explained in the ",(0,t.jsx)(n.a,{href:"combining",children:"Combining Planning & RL Agents"})," section."]}),"\n",(0,t.jsx)(n.h2,{id:"reward-function",children:"Reward Function"}),"\n",(0,t.jsxs)(n.p,{children:["The reward is generated from the observation module by the ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/blob/36f78f5e25475a43a8a83627939a5744d0a42c0c/envs/single_agent_standard.py",children:"SingleAgentWrapper"})," and passed to the external RL agent."]}),"\n",(0,t.jsxs)(n.p,{children:["Additional reward is generated in the reward-shaping and neurosymbolics wrappers. These are found in the ",(0,t.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym/tree/main/envs",children:"envs"})," folder and described in detail in the ",(0,t.jsx)(n.a,{href:"combining",children:"Combining Planning & RL Agents"})," section."]})]})}function d(e={}){const{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},1837:(e,n,i)=>{i.d(n,{Z:()=>t});const t=i.p+"assets/images/Observations.drawio-c38e08243c7ec515815cbea3184b5854.png"},1151:(e,n,i)=>{i.d(n,{Z:()=>a,a:()=>r});var t=i(7294);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);
"use strict";(self.webpackChunkng_website=self.webpackChunkng_website||[]).push([[316],{1016:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>l,frontMatter:()=>r,metadata:()=>s,toc:()=>d});var o=t(5893),i=t(1151);const r={sidebar_position:1},a="Keyboard Demo",s={id:"environment/demo",title:"Keyboard Demo",description:"To gain an intuition behind the game the agent will be taught to play, run the following command from the root of the NovelGym repository.",source:"@site/docs/environment/demo.md",sourceDirName:"environment",slug:"/environment/demo",permalink:"/docs/environment/demo",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Customizing Environments",permalink:"/docs/category/customizing-environments"},next:{title:"Creating Your Environment",permalink:"/docs/environment/custom"}},c={},d=[];function h(e){const n={a:"a",code:"code",h1:"h1",img:"img",p:"p",pre:"pre",strong:"strong",...(0,i.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"keyboard-demo",children:"Keyboard Demo"}),"\n",(0,o.jsxs)(n.p,{children:["To gain an intuition behind the game the agent will be taught to play, run the following command from the root of the ",(0,o.jsx)(n.a,{href:"https://github.com/tufts-ai-robotics-group/NovelGym",children:"NovelGym"})," repository."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"python manual_novelty_test1.py --novelty none --env sa\n"})}),"\n",(0,o.jsx)(n.p,{children:"A screen similar to the one below will pop up, rendering the environment and the agent navigating it."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Environment",src:t(493).Z+"",width:"553",height:"376"})}),"\n",(0,o.jsxs)(n.p,{children:["The agent, represented by a red arrow, can be operated by typing the integer corresponding to the desired action next to the prompt ",(0,o.jsx)(n.code,{children:"action: "})," generated on the command line. For instance, ",(0,o.jsx)(n.code,{children:"action: 2"}),", corresponding to ",(0,o.jsx)(n.code,{children:"approach_oak_log"}),", would make the agent approach the nearest oak log."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Note"}),": The ",(0,o.jsx)(n.code,{children:"--novelty none"})," flag in the command above means that we are trying out the environment without any novelty being introduced to it. Feel free to experiment by modifying this to any novelty listed when running the script with the ",(0,o.jsx)(n.code,{children:"--help"})," flag. On the other hand, the ",(0,o.jsx)(n.code,{children:"--env sa"})," represents the choice of the single agent wrapper. Throughout the ",(0,o.jsx)(n.a,{href:"../category/customizing-environments",children:"Customizing Environments"})," part of the tutorial, this will be the environment we default to; for a deeper dive into agent wrappers, see the tutorial on ",(0,o.jsx)(n.a,{href:"../agent/combining",children:"Combining Planning & RL Agents"}),"."]})]})}function l(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},493:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/environment-cfb98f70ad1a7ac41a21da8d5b359aba.png"},1151:(e,n,t)=>{t.d(n,{Z:()=>s,a:()=>a});var o=t(7294);const i={},r=o.createContext(i);function a(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);